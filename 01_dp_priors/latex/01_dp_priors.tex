% To compile: pdflatex file.tex
\documentclass{article}
\usepackage{fullpage}
\usepackage{pgffor}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{verbatim}
\usepackage{appendix}
\usepackage{graphicx}
\usepackage{url} % for underscore in footnote
\usepackage[UKenglish]{isodate} % for: \today
\cleanlookdateon                % for: \today

\def\wl{\par \vspace{\baselineskip}\noindent}
\def\beginmyfig{\begin{figure}[htbp]\begin{center}}
\def\endmyfig{\end{center}\end{figure}}
%\def\prodl{\prod\limits_{i=1}^n}
\def\suml{\sum\limits_{i=1}^n}
\def\ds{\displaystyle}
\def\tu{\textunderscore}

\begin{document}
% my title:
\begin{center}
  %\section*{\textbf{Stat637 Homework 8}
  %  \footnote{https://github.com/luiarthur/Fall2014/blob/master/Stat637/8}
  %}  
  \section*{\textbf{UCSC AMS 241 Homework 1 - DP Priors}
    \footnote{\url{https://github.com/luiarthur/bnp_hw/01_dp_priors}}
  } 
  \subsection*{\textbf{Arthur Lui}}
  \subsection*{\noindent\today}
\end{center}

\noindent
\section{Correlation of $G(B_1)$ and $G(B_2)$}
Assume a Dirichlet process (DP) prior, DP($\alpha,G_0$), for distributions $G$
on $\mathcal X$ . Show that for any (measurable) disjoint subsets $B_1$ and
$B_2$ of $\mathcal X$ , Corr($G(B_1),G(B_2)$) is negative. Is the negative
correlation for random probabilities induced by the DP prior a restriction?
Discuss.\\

\noindent $Proof:$\\

\noindent
Let $B_3$ = $(B_1 \cap B_2)^C$, and $U_i = G(B_i)$, for $i=1,2,3$.
Furthermore, let $v_i = G_0(B_i)$, for $i=1,2,3$. Then, Corr($G(B_1),G(B_2)$) =
Corr($U_1,U_2$). Note that Sign(Corr($U_1,U_2$)) = Sign(Cov($U_1,U_2$)). So,
all we need to do is show that the covariance is negative. Finally, let
$f(u_1,u_2,u_3)$ be the pdf for the Dirichlet distribution, with parameters
$(\alpha v_1, \alpha v_2, \alpha v_3)$. \\
\[
  \begin{array}{rcl}
                            \vspace{.5em}
    \text{Cov}(U_1,U_2) &=& \text{E}[U_1 U_2] - \text{E}[U_1]\text{E}[U_2] \\
                            \vspace{.5em}
                        &=& \ds\int\int\int u_1u_2~f(u_1,u_2,u_3)~du_3~du_2~du_1 - v_1 v_2 \\
                            \vspace{.7em}
                        &=& \ds\int u_1u_2~
                            \frac{\Gamma(\alpha v_1 + \alpha v_2 + \alpha v_3)}
                            {\Gamma(\alpha v_1)\Gamma(\alpha v_2)\Gamma(\alpha v_3)}
                            u_1^{\alpha v_1 - 1} u_2^{\alpha v_2 - 1} u_3^{\alpha v_3 - 1}
                            ~d\mathbf u - v_1 v_2 \\
                            \vspace{.5em}
                        &=& \ds\frac{\Gamma(\alpha v_1 +1)\Gamma(\alpha v_2 +1)}
                            {\Gamma(\alpha v_1 + \alpha v_2 + \alpha v_3 + 2)}
                            \ds\frac {\Gamma(\alpha v_1 + \alpha v_2 + \alpha v_3)}
                            {\Gamma(\alpha v_1)\Gamma(\alpha v_2)} \times\\
                            \vspace{.9em}
                        &&  \ds\int\frac{\Gamma(\alpha v_1 + \alpha v_2 + \alpha v_3 + 2)}
                            {\Gamma(\alpha v_1+1)\Gamma(\alpha v_2+1)\Gamma(\alpha v_3)}
                            u_1^{(\alpha v_1 + 1) - 1} u_2^{(\alpha v_2 + 1) - 1} u_3^{\alpha v_3 - 1}
                            ~d\mathbf u - v_1 v_2 \\
                            \vspace{.7em}
                        &=& \ds\frac{\Gamma(\alpha v_1 +1)\Gamma(\alpha v_2 +1)}
                            {\Gamma(\alpha v_1 + \alpha v_2 + \alpha v_3 + 2)}
                            \ds\frac {\Gamma(\alpha v_1 + \alpha v_2 + \alpha v_3)}
                            {\Gamma(\alpha v_1)\Gamma(\alpha v_2)} - v_1 v_2\\
                            \vspace{.7em}
                        &=& \ds\frac{\Gamma(\alpha)}{\Gamma(\alpha+2)}
                            \ds\frac{\alpha v_1\Gamma(\alpha v_1) \cdot \alpha v_2\Gamma(\alpha v_2)}
                            {\Gamma(\alpha v_1)\Gamma(\alpha v_2)} - v_1 v_2\\
                            \vspace{.7em}
                        &=& \ds\frac{\alpha v_1 v_2}{(\alpha +1)} - v_1 v_2 \\
                            \vspace{.5em}
                        &=& v_1 v_2 \ds\frac{\alpha - \alpha - 1}{(\alpha +1)} \\
                            \vspace{.5em}
                        &=& -\ds\frac{v_1 v_2 }{(\alpha +1)} \\
                            \vspace{.5em}
                        &=& -\ds\frac{G_0(B_1) G_0(B_2) }{(\alpha +1)} \\
                        &<& 0 \\
  \end{array}
\]
Therefore, Corr($G(B_1),G(B_2)$) is negative. This result is intuitive and not necessarily a restriction.
We want it to be the case that as more weight is given to a partition $B_i$, then less weight is
given to the partition $B_j$, if $B_i$ and $B_j$ are disjoint.

\newpage
\section{Simulation of Dirichlet Process Prior Relizations}
Figure 1 shows draws from the DP using Setharuman's with varying $\alpha$
values. The $\alpha$ values from left to right are 1, 3, and 5. The mass
parameter $\alpha$ affects the point-wise variance of the cdf's drawn from the
DP. Notice that with small $\alpha$, the point-wise variance is larger at the
center of the DP draws; and with larger $\alpha$, the point-wise variance is
small. (This is illustrated by the red line.) The expected value of the DP is
the base-line distribution $G_0$, which is a standard Normal in our case.  To
confirm this, the point-wise means for the DP draws were taken and plotted in
blue. A few partiular draws from the DP are drawn in black, and the rest of the
draws are shaded in light grey such that the more likely draws are darker. \\

\noindent
Figure 2 shows draws from the DP using Ferguson's construction, again with
varying $\alpha$ values.  From left to right, the parameters are again 1, 3,
and 5. We see that there are no differences between the plots in Figure 1 and
2. This is expected as the 2 methods used to simulate draws from the DP should
yield the same distribution.

\vspace{5em}
\beginmyfig
  \includegraphics[scale=.4]{../code/pdfs/sethDP.pdf}
  \caption{DP draws using Setharuman's construction with varying $\alpha$
  values, with $\alpha=1, 3, \text{and } 5$ from left to right.}
\endmyfig
\beginmyfig
  \includegraphics[scale=.4]{../code/pdfs/fergusonDP.pdf}
  \caption{DP draws using Ferguson's construction with varying $\alpha$
  values, with $\alpha=1, 3, \text{and } 5$ from left to right.}
\endmyfig

Figure 3 shows draws from a mixture of Dirichlet process (MDP), which is
different from a Dirichlet process mixture model. Priors were placed on the
$\alpha$ parameter and draws from the DP were made by sampling from the priors
for $\alpha$ and then sampling from a DP($\alpha,G_0$)|$\alpha$, where $\alpha
\sim \Gamma(a,b)$ and $b$ is the scale parameter for the Gamma distribution.
Again, $G_0$ was chosen to be a standard Normal. The density for $\alpha$ was
plotted in orange. The $(a,b)$ parameters are chosen such that the means and
variances for $\alpha$ from the left plot to the right plot are (1,1), (5,1),
and (5,1000). When both the mean and variance for $\alpha$ are small (1,1), the
point-wise variance is large in the center of the DP draws. With the same
variance but a slightly larger mean (5,1), the point-wise variance is smaller.
This is expected as on average, $\alpha$ is larger. And we learned from Figures
1 and 2 that larger $\alpha$ yields smaller point-wise variance. It is
interesting that with a really large variance (1000) but a mean of only 5, the
point-wise variance is again large. This is because for the Gamma distribution,
more weight is given to $\alpha$ values closer to 0, but values far out in the
right tail can be drawn frequently. This leads to a combination of DP draws that
are both very discrete and only slightly discrete.
\beginmyfig
  \includegraphics[scale=.37]{../code/pdfs/priorMDP.pdf}
  \caption{Mixture Dirichlet Process Draws. For each of these plots, 
  $G|\alpha \sim \text{DP}(\alpha,G_0)$, and $\alpha \sim \text{Gamma}(a,b)$,
  where $a$ and $b$ are respectively the shape scale parameters of the 
  Gamma distribution.}
\endmyfig

\newpage
\section{Posterior Inference for One-sample Problems using DP Priors}
\beginmyfig
  \includegraphics[scale=.5,angle=270]{../code/pdfs/dppost1.pdf}
  \caption{Posterior inference for one-sample problems using 
  DP priors. Note that the first, second, and third rows of th plots are for
  sample sizes 20, 200, and 2000, respectively.  Also, the black, red, blue, and
  green lines represent the empirical cdf, $G_0(m,s)$, E$[G|y]$ and the true
  underlying cdf of the generated data.}
\endmyfig
\beginmyfig
  \includegraphics[scale=.5,angle=270]{../code/pdfs/dppost2.pdf}
  \caption{Posterior inference for one-sample problems using 
  DP priors. Note that the first, second, and third rows of th plots are for
  sample sizes 20, 200, and 2000, respectively.  Also, the black, red, blue, and
  green lines represent the empirical cdf, $G_0(m,s)$, E$[G|y]$ and the true
  underlying cdf of the generated data.}
\endmyfig


\newpage
\section{Posterior Inference for Count Data using MDP Priors}
\[
  \begin{array}{rclcl}
    y_i &|& F &\sim & F \\
    F &|& \alpha, \lambda &\sim& \text{DP}(\alpha,\text{Poisson}(\lambda)) \\
      && \lambda &\sim& \Gamma(a_\lambda,b_\lambda) \\
      && \alpha &\sim& \Gamma(a_\alpha,b_\alpha) \\
  \end{array}
\]
\beginmyfig
  \includegraphics[scale=.5]{../code/pdfs/postMDP1.pdf}
  \caption{Posterior inference for count data using MDP prior}
\endmyfig
\beginmyfig
  \includegraphics[scale=.5]{../code/pdfs/postMDP2.pdf}
  \caption{Posterior inference for count data using MDP prior}
\endmyfig


\end{document}
