% To compile: pdflatex file.tex
\documentclass{article}
\usepackage{fullpage}
\usepackage{pgffor}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{verbatim}
\usepackage{appendix}
\usepackage{graphicx}
\usepackage{url} % for underscore in footnote
\usepackage[UKenglish]{isodate} % for: \today
\cleanlookdateon                % for: \today

\def\wl{\par \vspace{\baselineskip}\noindent}
\def\beginmyfig{\begin{figure}[htbp]\begin{center}}
\def\endmyfig{\end{center}\end{figure}}
%\def\prodl{\prod\limits_{i=1}^n}
\def\suml{\sum\limits_{i=1}^n}
\def\ds{\displaystyle}
\def\tu{\textunderscore}

\begin{document}
% my title:
\begin{center}
  %\section*{\textbf{Stat637 Homework 8}
  %  \footnote{https://github.com/luiarthur/Fall2014/blob/master/Stat637/8}
  %}  
  \section*{\textbf{UCSC AMS 241 Homework 1 - DP Priors}
    \footnote{\url{https://github.com/luiarthur/bnp_hw/01_dp_priors}}
  } 
  \subsection*{\textbf{Arthur Lui}}
  \subsection*{\noindent\today}
\end{center}

\noindent
\section{Correlation of $G(B_1)$ and $G(B_2)$}
Assume a Dirichlet process (DP) prior, DP($\alpha,G_0$), for distributions $G$
on $\mathcal X$ . Show that for any (measurable) disjoint subsets $B_1$ and
$B_2$ of $\mathcal X$ , Corr($G(B_1),G(B_2)$) is negative. Is the negative
correlation for random probabilities induced by the DP prior a restriction?
Discuss.\\

\noindent $Proof:$\\

\noindent
Let $B_3$ = $(B_1 \cap B_2)^C$, and $U_i = G(B_i)$, for $i=1,2,3$.
Furthermore, let $v_i = G_0(B_i)$, for $i=1,2,3$. Then, Corr($G(B_1),G(B_2)$) =
Corr($U_1,U_2$). Note that Sign(Corr($U_1,U_2$)) = Sign(Cov($U_1,U_2$)). So,
all we need to do is show that the covariance is negative. Finally, let
$f(u_1,u_2,u_3)$ be the pdf for the Dirichlet distribution, with parameters
$(\alpha v_1, \alpha v_2, \alpha v_3)$. \\
\[
  \begin{array}{rcl}
                            \vspace{.5em}
    \text{Cov}(U_1,U_2) &=& \text{E}[U_1 U_2] - \text{E}[U_1]\text{E}[U_2] \\
                            \vspace{.5em}
                        &=& \ds\int\int\int u_1u_2~f(u_1,u_2,u_3)~du_3~du_2~du_1 - v_1 v_2 \\
                            \vspace{.7em}
                        &=& \ds\int u_1u_2~
                            \frac{\Gamma(\alpha v_1 + \alpha v_2 + \alpha v_3)}
                            {\Gamma(\alpha v_1)\Gamma(\alpha v_2)\Gamma(\alpha v_3)}
                            u_1^{\alpha v_1 - 1} u_2^{\alpha v_2 - 1} u_3^{\alpha v_3 - 1}
                            ~d\mathbf u - v_1 v_2 \\
                            \vspace{.5em}
                        &=& \ds\frac{\Gamma(\alpha v_1 +1)\Gamma(\alpha v_2 +1)}
                            {\Gamma(\alpha v_1 + \alpha v_2 + \alpha v_3 + 2)}
                            \ds\frac {\Gamma(\alpha v_1 + \alpha v_2 + \alpha v_3)}
                            {\Gamma(\alpha v_1)\Gamma(\alpha v_2)} \times\\
                            \vspace{.9em}
                        &&  \ds\int\frac{\Gamma(\alpha v_1 + \alpha v_2 + \alpha v_3 + 2)}
                            {\Gamma(\alpha v_1+1)\Gamma(\alpha v_2+1)\Gamma(\alpha v_3)}
                            u_1^{(\alpha v_1 + 1) - 1} u_2^{(\alpha v_2 + 1) - 1} u_3^{\alpha v_3 - 1}
                            ~d\mathbf u - v_1 v_2 \\
                            \vspace{.7em}
                        &=& \ds\frac{\Gamma(\alpha v_1 +1)\Gamma(\alpha v_2 +1)}
                            {\Gamma(\alpha v_1 + \alpha v_2 + \alpha v_3 + 2)}
                            \ds\frac {\Gamma(\alpha v_1 + \alpha v_2 + \alpha v_3)}
                            {\Gamma(\alpha v_1)\Gamma(\alpha v_2)} - v_1 v_2\\
                            \vspace{.7em}
                        &=& \ds\frac{\Gamma(\alpha)}{\Gamma(\alpha+2)}
                            \ds\frac{\alpha v_1\Gamma(\alpha v_1) \cdot \alpha v_2\Gamma(\alpha v_2)}
                            {\Gamma(\alpha v_1)\Gamma(\alpha v_2)} - v_1 v_2\\
                            \vspace{.7em}
                        &=& \ds\frac{\alpha v_1 v_2}{(\alpha +1)} - v_1 v_2 \\
                            \vspace{.5em}
                        &=& v_1 v_2 \ds\frac{\alpha - \alpha - 1}{(\alpha +1)} \\
                            \vspace{.5em}
                        &=& -\ds\frac{v_1 v_2 }{(\alpha +1)} \\
                            \vspace{.5em}
                        &=& -\ds\frac{G_0(B_1) G_0(B_2) }{(\alpha +1)} \\
                        &<& 0 \\
  \end{array}
\]
Therefore, Corr($G(B_1),G(B_2)$) is negative. This result is intuitive and not necessarily a restriction.
We want it to be the case that as more weight is given to a partition $B_i$, then less weight is
given to the partition $B_j$, if $B_i$ and $B_j$ are disjoint.

\newpage
\section{Simulation of Dirichlet Process Prior Relizations}
\beginmyfig
  \includegraphics[scale=.4]{../code/pdfs/sethDP.pdf}
  \caption{Dirichlet Process Draws using Setharuman's Construction}
\endmyfig
\beginmyfig
  \includegraphics[scale=.4]{../code/pdfs/fergusonDP.pdf}
  \caption{Dirichlet Process Draws using Ferguson's Construction}
\endmyfig
\beginmyfig
  \includegraphics[scale=.4]{../code/pdfs/priorMDP.pdf}
  \caption{Mixture Dirichlet Process Draws. For each of these plots, 
  $G|\alpha \sim \text{DP}(\alpha,G_0)$, and $\alpha \sim \text{Gamma}(a,b)$,
  where $a$ and $b$ are respectively the shape scale parameters of the 
  Gamma distribution.}
\endmyfig

\newpage
\section{Posterior Inference for One-sample Problems using DP Priors}
\beginmyfig
  \includegraphics[scale=.5,angle=270]{../code/pdfs/dppost1.pdf}
  \caption{Posterior inference for one-sample problems using 
  DP priors. Note that the first, second, and third rows of th plots are for
  sample sizes 20, 200, and 2000, respectively.  Also, the black, red, blue, and
  green lines represent the empirical cdf, $G_0(m,s)$, E$[G|y]$ and the true
  underlying cdf of the generated data.}
\endmyfig
\beginmyfig
  \includegraphics[scale=.5,angle=270]{../code/pdfs/dppost2.pdf}
  \caption{Posterior inference for one-sample problems using 
  DP priors. Note that the first, second, and third rows of th plots are for
  sample sizes 20, 200, and 2000, respectively.  Also, the black, red, blue, and
  green lines represent the empirical cdf, $G_0(m,s)$, E$[G|y]$ and the true
  underlying cdf of the generated data.}
\endmyfig


\newpage
\section{Posterior Inference for Count Data using MDP Priors}


\end{document}
