% To compile: pdflatex file.tex
\documentclass{article}
\usepackage{fullpage}
\usepackage{pgffor}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{verbatim}
\usepackage{appendix}
\usepackage{graphicx}
\usepackage{color}
\usepackage{subfig}
\usepackage{url} % for underscore in footnote
\usepackage[UKenglish]{isodate} % for: \today
\cleanlookdateon                % for: \today
%\pagestyle{empty} % Removes page number. Graphs too big.

\def\wl{\par \vspace{\baselineskip}\noindent}
\def\beginmyfig{\begin{figure}[h]\center}
\def\endmyfig{\end{figure}}
\def\prodl{\prod\limits_{j=1}^{n^*}}
\def\suml{\sum\limits_{i=1}^n}
\def\ds{\displaystyle}
\def\tu{\textunderscore}
\definecolor{grey}{rgb}{.2,.2,.2}
\definecolor{lgrey}{rgb}{.8,.8,.8}
\def\hline{ \textcolor{lgrey}{\hrulefill} }
\newcommand{\m}[1]{\mathbf{\bm{#1}}} % Serif bold math

% \def for THIS ASSIGNMENT!!!%%%%%%%%%%%%%%%%%%%
\def \ip{{i^\prime}}
\def \tmi{ \{ \theta_\ip : \ip \neq i\} }
\def \stmi{ \m{\theta}_{-i} }

\def \tv{ \{ \theta_i : i = 1,...,n\} }
\def \stv{ \m{\theta} }

\def \tu{ \{ \theta_j^* : i = 1,...,n^*\} }
\def \stu{ \m{\theta^*} }

\def \y {\m{y}}

\begin{document}
% my title:
\begin{center}
  {\huge \textbf{Dirichlet Process Mixture Models}
    \footnote{\url{https://github.com/luiarthur/bnp_hw/02_dp_priors}}
  }\\
  \wl
  UCSC AMS 241 Homework 2\\
  \noindent\today\\
  Arthur Lui\\
  \hline
\end{center}

\section{Location Normal DP Mixed Model}
\subsection{P\'olya urn based Gibbs sampler}
\def\denom{ \alpha q_0 + \sum_{j=1}^{n^{*-}}n_j^- q_j }
Below is P\'olya urn based Gibbs sampler.
I will use the following notation for convenience: $\stv = \tv$, $\stmi = \tmi$, and $\stu = \tu$.
\[
    p(\theta_i | \stmi, \alpha, \phi, \mu, \tau^2, y_i) = 
    \ds\frac{\alpha q_0}{\denom} h(\theta_i| \psi, \phi, y_i) + 
    \ds\sum_{j=1}^{n^{*-}}\frac{ n_j^- q_j }{\denom} \delta_{\theta_j^{*-}}(\theta_i)
\]
where 
\begin{itemize}
  \item $q_j = k(y_i | \theta_j^{*-},\phi)$, the Normal density with mean and variance $\theta_j^{*-}$ and $\phi$ 
    evaluated at $y_i$.
  \item $q_0 = \ds\int k(y_i | \theta, \phi) g_0(\theta | \mu, \tau^2) d\theta
    = k(y_i~|~\mu,~\tau^2 + \phi)$
  \item $h(\theta_i| \psi, \phi, y_i) = c \cdot k(y_i | \theta_i, \phi) g_0(\theta_i | \mu, \tau^2)
    = k\left(\theta_i~\biggr|~\ds \frac{y_i\tau^2 + \mu\phi}{\tau^2+\phi},
    \ds\frac{\tau^2\phi}{\tau^2 + \phi} \right)$, where $c$ is a constant.
  \item $g_0$, is the density of $G_0$ and is $k(\theta | \mu, \tau^2)$.
\end{itemize}


\subsection{Model \& Prior Specification}
The hierearchical version of the DP mixture model of interest is 
\[
\begin{array}{rclcll}
  y_i &|& \theta_i,\phi &\overset{ind.}{\sim}& \text{N}(\theta_i, \phi), & i = 1,...,n\\
  \theta_i &|& G &\overset{i.i.d.}{\sim}&  G, & i = 1,...,n\\
  G &|& \alpha, \mu, \tau^2 &\sim& \text{DP}(\alpha, G_0=\text{N}(\mu,\tau^2))\\
    && \phi &\sim& \Gamma^{-1}(a_\phi,b_\phi) \\
    && \alpha &\sim& \Gamma(a_\alpha,b_\alpha) \\
    && \mu &\sim& \text{N}(a_\mu,b_\mu) \\
    && \tau^2 &\sim& \Gamma^{-1}(a_{\tau^2},b_{\tau^2}).\\
\end{array}
\]
where $\Gamma^{-1}(a,b)$ denotes the inverse-Gamma distribution with the associated density 
$f(x) = \ds \frac{b^a}{\Gamma(a)} x^{-a-1} \exp\left(\frac{-b}{x}\right)$, and we use the
shape and \emph{rate} parameterization for the Gamma distribution.

\noindent
We can write the complete conditions in closed form to be used in a Gibbs sampler as follows.
\[
\begin{array} {rclcl}
  \phi &|& \stv, \y &\sim& \Gamma^{-1}\left(\ds\frac{n}{2}+a_\phi,\frac{\suml (y_i-\theta_i)^2}{2} + b_\phi \right)\\
  \\
  \mu &|& \stu, n^*, \y &\sim& \text{N}\left(\ds \frac{(n^*\bar{\stu}) b_\mu + a_\mu\tau^2}{n^*b_\mu + \tau^2},
    \ds\frac{b_\mu \tau^2}{n^* b_\mu + \tau^2} \right)\\
  \\
  \tau^2 &|& \stu, n^*, \y &\sim& \Gamma^{-1}\left(\ds\frac{n^*}{2}+a_{\tau^2},
    \frac{\sum_{j=1}^{n^*} (\theta_j^*-\mu)^2}{2} + b_{\tau^2} \right)\\
  \eta &|& \alpha, \y &\sim& \text{Beta}(\alpha+1,n) \\
  p(\alpha &|& n^*, \y) &=& (\epsilon)~\gamma(\alpha | a_\alpha+n^*, b_\alpha-\log(\eta)) +
    (1-\epsilon)~\gamma(\alpha | a_\alpha+n^*-1, b_\alpha-\log(\eta))\\
\end{array}
\]
where $\gamma$ is the gamma density function with the mean and rate parameterization, 
$\epsilon = \ds\frac{a_\alpha +n^* - 1}{ n(b_\alpha-\log(\eta)) + a_\alpha +n^* -1}$,
and $\eta$ is an auxiliary variable introduced to make the prior for $\alpha$ conjugate.\\ % In Lecture Notes 2.

\noindent
For $\phi$, an inverse gamma prior with parameters (3,3) were chosen. This
corresponds to a mean and variance of 1.5 and 2.6. $\phi$ represents the
variance of the data which share the same mean. This should typically be small
compared to the variance of the entire data set. Here, the data set has a
variance of 9. $\mu$, the mean parameter of the centering distribution was
given a Normal prior with mean and variance 0 and 9.  The data set has a
variance of 9 and mean of -.27.  $\tau^2$ corresponds to the variance of the
centering distribution and has an inverse gamma (3,6) prior, which has
mean and variance 3 and 9.5. The variation between different clusters
may be large, so I chose a large variance, but kept the mean close to 0.\\

%a_alpha, b_alpha = 2,4 # Shape and Rate a*log(1+n/a) = 1.5
%a_phi, b_phi = 3,3#3,3 # mean 1.5, var 2.6
%a_mu, b_mu = 0,3 # 0,3
%a_t2, b_t2 = 3,15#3,6 # mean 3, var 9.5

\subsection{Posterior Sensitivity}
We can examine the sensitivity of the model by altering the priors for the
hyperparameters. Table \ref{tab:posts1} shows the sensitivity of out model to
the prior specifications. The study was done by changing only one prior at a
time leaving the other priors unchanged. The new priors used were chosen to be
more extreme, and were $\phi \sim \text{IG}(3,15)$, $\mu \sim \text{N}(10,3)$,
and $\tau^2 \sim \text{IG}(3,15)$. These correspond to increasing the prior
means of the parameters to be more extreme. The posterior distributions were
not greatly affected by moderate increases in $\phi$ and $\tau^2$, but were
greatly influenced by changing the prior of $\mu$. The posterior mean for $\mu$
increased from just below -.3 to 6.66. And $\tau^2$ increased to 30.4 from 6.7.
This is logical because the centering distribution requires a higher variance 
when $\mu$ is poorly specified.

\beginmyfig
  \includegraphics[scale=.5]{img/posts1.pdf}
  \caption{Posterior for Hyperparameters}
  \label{fig:post1}
\endmyfig

\begin{table}[h]
  \center
  \begin{tabular}{r|rrrr}
    Parameters & Original & Large E[$\phi$]& Large E[$\mu$] & Large E[$\tau^2$] \\
    $\phi$   &  .90 & 1.1  &   .97 &  .88 \\
    $\mu$    & -.30 & -.27 &  6.66 & -.25 \\
    $\tau^2$ & 6.71 & 6.86 & 30.4  & 8.74 \\
  \end{tabular}
  \caption{Sensitivity analysis: table of how the hyperparameters' posterior
  means change as the priors are changed.}
  \label{tab:posts1}
\end{table}

%\begin{figure}
%  \center
%  \subfloat[Original Model]{ \includegraphics[scale=.3]{img/posts1.pdf} } 
%  \hfill
%  \subfloat[Large $\phi$]{ \includegraphics[scale=.3]{img/posts1HighPhi.pdf} } \\
%  \subfloat[Large $\mu$]{ \includegraphics[scale=.3]{img/posts1HighMu.pdf} } 
%  \hfill
%  \subfloat[Large $\tau^2$]{ \includegraphics[scale=.3]{img/posts1HighT2.pdf} }
%\end{figure}

\noindent
The posterior predictive distributions are plotted in Figure
\ref{fig:postpreds1} for the various priors. They appear to be similar.\\
\begin{figure}[h]
  \center
  \subfloat[Original Model]{ \includegraphics[scale=.4]{img/postpred1.pdf} } 
  \hfill
  \subfloat[Large $\phi$]{ \includegraphics[scale=.4]{img/postpred1HighPhi.pdf} } \\
  \subfloat[Large $\mu$]{ \includegraphics[scale=.4]{img/postpred1HighMu.pdf} } 
  \hfill
  \subfloat[Large $\tau^2$]{ \includegraphics[scale=.4]{img/postpred1HighT2.pdf} }
  \caption{Posterior predictives under various priors.}
  \label{fig:postpreds1}
\end{figure}


\subsection{Posterior Distribution for $\alpha$ and $n^*$}
The prior distribution for $\alpha$ is Gamma(2,4), which corresponds to an
expected number of clusters of $2/4 * log(1 + 250 /(2/4) ) \approx 3$.  The
posterior distribution of $n^*$ is shown in figure \ref{fig:nstar1}.  The mean
and variance of $n^*$ are 6.3 and 5.4 respectively. We change the prior to
Gamma(2,.1) to observe the sensitivity of the posterior to the prior. The mean
and variance of the number of clusters $n^*$ are 28 and 143 respectively.
$\alpha$ is similarly affected when the prior become more extreme.  The
posterior mean for $\alpha$ increases from .85 to 8.8 when when the rate term
in the Gamma prior is changed from 4 to .1, (which corresponds to an increase
from 3 to 48, in the expected number of clusters. Both $\alpha$ and $n^*$ are
sensitive to the prior on $\alpha$. The posteriors for $n^*$ (for both priors)
and $\alpha$ (for only the more extreme prior) are plotted in Firgure
\ref{fig:nstar1}. 
\beginmyfig
  \subfloat{\includegraphics[scale=.5]{img/postnstar1.pdf}}
  \subfloat{\includegraphics[scale=.5]{img/nstar1big.pdf}}\\
  \subfloat{\includegraphics[scale=.3]{img/posts1alpha.pdf}}
  \caption{Posterior for $n^*$ for $\alpha \sim \text{Gamma}(2,4)$ (left) and
  $\alpha \sim \text{Gamma}(2,.1)$ (right). The priors for $\alpha$ have a strong
  influence on the posterior number of clusters.}
  \label{fig:nstar1}
\endmyfig


\subsection{Clustering Induced by DP Mixture}
Figure \ref{fig:clus1} shows the posterior medians of $\m\theta$ sorted by the
magnitude of $y$ in blue, the 2.5-97.5\% quantiles in pink, and the data in
grey.  There appears to be three clusters of $\theta$'s with medians at -5, 0,
and 3.5. The length of the three blue lines also provide information about the
proportion of $\theta$'s that belong to a certain group. For example, the first
(approximately) 50 $\theta$'s out of 250 have median -5. That is 20\% of the
observations have mean -5. The next 125 observations (or 50\% of the
observations) have mean 0. The last 30\% of the observations have mean 3.5.
This is consistent with the true model.  \\
\beginmyfig
  \includegraphics[scale=.5]{img/clus1.pdf}
  \caption{Data shown in grey, posterior median of $\m\theta$ sorted
  by magnitude of $y$ in blue, and 2.5-97.5\% quantiles in pink.
  There appears to be three clusters with means at -5, 0, and 3.5.
  The length of the three lines also provide information about 
  the proportion of $\theta$'s that belong to a certain group.}
  \label{fig:clus1}
\endmyfig


\subsection{Posterior Predictive}
Figure \ref{fig:postpred1} shows the posterior predictive density for the
$\m\theta$'s in blue, along with the density of the data in grey, the true
distribution of the data in green, the posterior predictive of the
location-scale model (which will be described in the following section) in red,
and the prior predictive density in yellow.  The model was able to recover the
underlying distribution of the data. This can be seen from the posterior
predictive density following the true density closer than the data. The prior
predictive shows that the priors are well specified, while it does not generate
the shape of the true distribution, as it covers the space of the data.

% Problem 2: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Location-Scale Normal DP Mixture Model}
\subsection{Prior Specification}
For the location-scale DP mixture, I chose the following priors:
$\mu\sim\text{Normal}(0,3^2) $, $\kappa\sim\text{Gamma}(3,3)$,$\beta\sim\text{InverseGamma}(2,1)$
%prior2 <- list(a0=3,b0=3,m2=0,s2=3,psiinv2=solve(diag(.5,1)),nu1=1,nu2=2,tau1=6,tau2=6)
The prior for $\mu$ was chosen such that the mean is 0 and the variance is 9 to account for 
large uncertainty. The prior for $\kappa$ was chosen such that the mean is 1 and variance is 1/3
; the prior for $\beta$ was chosen such that the mean is also 1. Together, the mean of $\phi/\kappa$
will be one apriori.
\subsection{Model Comparison}
Figure \ref{fig:postpred1} shows the posterior predictive for the location-scale model
along with the that of the location model. By observation only, it does not perform better
than the location model. This is not surprising as the scales are constant across the different 
clusters. In fact, it appears that the location scale model is closer to the data than the 
actual model, while the location only DP mixture is closer to the truth than the data. The 
location-scale model may be over-fitting when there are no true underlying clusters in the
scales, but only one scale shared across different groups.
\beginmyfig
  \includegraphics[scale=.5]{img/postpred1.pdf}
  \caption{Posterior predictive distribution in blue, density of data in grey, the true
  underlying distribution which generated the data in green, posterior predictive of
  location-scale model in red, and prior predictive in yellow.}
  \label{fig:postpred1}
\endmyfig

% Problem 3: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{DP Mixture-of-Poissons Regression}
\subsubsection{Model}
\end{document}

% To compile to html: 
% $ pandoc -s --mathjax -o hw2.html 02_dpmm.tex
% -o for output, -s for stand alone, --mathjax for mathjax
% For more details: http://pandoc.org/demos.html
